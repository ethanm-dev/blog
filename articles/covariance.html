<!DOCTYPE HTML>
<html>
    <head>
        <title>Single-pass, exponentially decayed covariance in Python</title>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        
        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/default.min.css">

        <link rel="stylesheet" href="../style.css">

        <style>
            code {
                font-family: "Source Code Pro", monospace;
                font-optical-sizing: auto;
                font-weight: 400;
                font-style: normal;
                font-size: 1rem;
            }
        </style>
    </head>
    <body>
        <div id="wrapper">

            <!-- Title-->
            <section class="article-title">
                <h1>Single-pass, exponentially decayed covariance in Python</h1>
                <p class="major">
                    Given vectors of timestamped observations of two jointly distributed random variables, this article
                    derives & discusses the Python implementation of a single-pass algorithm to compute an exponentially
                    decayed covariance of these observations.
                </p>
                <a id="home-button" href="../index.html">&lt;&nbsp;&nbsp;Home</a>
            </section>

            <!-- Content -->
            <section class="article-body">
                <h3>Context</h3>
                <p>
                    Suppose there are two jointly distributed (i.e, not independent) random variables $X$ and $Y$, for which
                    we have corresponding vectors of observations $\boldsymbol{x}$ and $\boldsymbol{y}$, recorded at epoch
                    timestamps $\boldsymbol{t}$ seconds. We wish to measure a weighted sample covariance between these
                    observations, with weights derived from observation times, where the most recent observations have the
                    largest weights. For performance reasons, our algorithm should only utilise a "single pass", meaning we
                    iterate through the vectors $\boldsymbol{x}$, $\boldsymbol{y}$ and $\boldsymbol{t}$ only once.
                </p>

                <h3>Definitions</h3>
                <p>
                    The unweighted sample covariance (in $(-\infty,+\infty )$) of these observations is defined as:
                    $$\text{Cov}(\boldsymbol{x}, \boldsymbol{y}) \> = \> \frac{\sum_{i=1}^N (x_i - x^*)(y_i - y^*)}{N-1}$$
                    where $N$ is the number of observations, $x^*$ is the mean of $\boldsymbol{x}$ and $y^*$ is the mean of
                    $\boldsymbol{y}$. This quantifies the relationship between the variances of $\boldsymbol{x}$ and $\boldsymbol{y}$:
                </p>
                <ul>
                    <li>a positive covariance indicates that $X$ and $Y$ tend to move together,</li>
                    <li>a negative covariance indicates that they move in opposite directions, and</li>
                    <li>zero covariance suggests that $X$ and $Y$ are actually independent.</li>
                </ul>
                <p>
                    To implement "exponential decay" using the non-decreasing observation times $\boldsymbol{t}$, given a
                    half life of $T$ seconds, we define observation weights:
                    $$w_i \> = \> 2^{-\frac{t_N - t_i}{T}}$$
                    for $i \in [1, N]$, such that $w_N = 1$, $w_i = 0.5$ when $t_N - t_i = T$ (hence the name "half life")
                    and $w_i \to 0$ as $t_i$ decreases. These weights are then used to compute weighted means of $\boldsymbol{x}$
                    and $\boldsymbol{y}$:
                    $$x^* = \frac{\sum_{i=1}^N w_i x_i}{\sum_{i=1}^N w_i}, \>\>\> y^* = \frac{\sum_{i=1}^N w_i y_i}{\sum_{i=1}^N w_i}.$$
                    Using these, we can define the biased, weighted covariance of $\boldsymbol{x}$ and $\boldsymbol{y}$ as:
                    $$\text{Cov}_B(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{w}) \> = \> \frac{\sum_{i=1}^N w_i (x_i - x^*)(y_i - y^*)}{\sum_{i=1}^N w_i},$$
                    and the unbiased, weighted covariance as:
                    $$\text{Cov}_U(\boldsymbol{x}, \boldsymbol{y}, \boldsymbol{w}) \> = \> \frac{\sum_{i=1}^N w_i}{\left( \sum_{i=1}^N w_i \right) - \sum_{i=1}^N w_i^2} \sum_{i=1}^N w_i (x_i - x^*)(y_i - y^*).$$
                    To help convince yourself of this definition, try substituting uniform weights of $w_i = 1$. The above
                    expression will reduce to the unweighted sample covariance we defined at the beginning of this section.
                </p>

                <h3>Derivation</h3>
                <p>
                    In order to implement a single-pass algorithm, we need to derive recursive formulas for the weighted
                    means and covariance. Let $W_n \> = \> \sum_{i=1}^n w_i$ be the sum of the first $n$ weights in
                    $\boldsymbol{w}$. Then the weighted mean of the first $n$ observations of $X$ is
                    $$\begin{aligned}
                        \mu_n \> &= \> \frac{1}{W_n} \sum_{i=1}^n w_i x_i \\
                        &= \> \frac{1}{W_n} \left( \sum_{i=1}^{n-1} w_i x_i + w_n x_n \right) \\
                        &= \> \frac{1}{W_n} \left( \frac{W_{n-1}}{W_{n-1}} \sum_{i=1}^{n-1} w_i x_i + w_n x_n \right) \\
                        &= \> \frac{W_{n-1}}{W_n} \left( \frac{1}{W_{n-1}} \sum_{i=1}^{n-1} w_i x_i \right) + \frac{w_n}{W_n} x_n \\
                        &= \> \frac{W_{n-1}}{W_n} \mu_{n-1} + \frac{w_n}{W_n} x_n \\
                        &= \> \left( 1 - \frac{w_n}{W_n} \right) \mu_{n-1} + \frac{w_n}{W_n} x_n,
                    \end{aligned}$$
                    thus we have a formula for the $n^{\text{th}}$ weighted mean as a linear combination of the
                    $(n-1)^{\text{th}}$ weighted mean and the $n^{\text{th}}$ observation.
                </p>
                <p>
                    Next, we look at deriving Welford's method for computing weighted variance (a specific case of weighted
                    covariance) in a single pass. We define the "sum of weighted squared deviations" of the first $n$
                    observations of $X$ as
                    $$\beta_n \> = \> \sum_{i=1}^n w_i (x_i - \mu_n)^2$$
                    such that the biased, weighted variance $\sigma_n^2 = \frac{\beta_n}{W_n}$. Consider the difference
                    between consecutive sums:
                    $$\begin{aligned}
                        \beta_n - \beta_{n-1} \> &= \> \sum_{i=1}^n w_i (x_i - \mu_n)^2 - \sum_{i=1}^{n-1} w_i (x_i - \mu_{n-1})^2 \\
                        &= \> w_n(x_n - \mu_n)^2 + \sum_{i=1}^{n-1} w_i \left( (x_i - \mu_n)^2 - (x_i - \mu_{n-1})^2 \right) \\
                        &= \> w_n(x_n - \mu_n)^2 + \sum_{i=1}^{n-1} w_i \left( (x_i - \mu_n - x_i + \mu_{n-1}) (x_i - \mu_n + x_i - \mu_{n-1}) \right) \\
                        &= \> w_n(x_n - \mu_n)^2 + \sum_{i=1}^{n-1} w_i \left( (\mu_{n-1} - \mu_n) (x_i - \mu_n + x_i - \mu_{n-1}) \right) \\
                        &= \> w_n(x_n - \mu_n)^2 + (\mu_{n-1} - \mu_n) \sum_{i=1}^{n-1} w_i \left( (x_i - \mu_n) + (x_i - \mu_{n-1}) \right).
                    \end{aligned}$$
                    To continue simplifying this expression, we use the fact that the weighted sum of deviations from
                    the mean is zero:
                    $$
                        \sum_{i=1}^k w_i(x_i - \mu_k) \> = \> \sum_{i=1}^k w_i x_i - \mu_k \sum_{i=1}^k w_i \>
                        = \> \sum_{i=1}^k w_i x_i - \left( \frac{\sum_{i=1}^k w_i x_i}{\sum_{i=1}^k w_i} \right) \sum_{i=1}^k w_i \>
                        = \> \sum_{i=1}^k w_i x_i - \sum_{i=1}^k w_i x_i \> = \> 0.
                    $$
                    Thus, we have that
                    $$\begin{aligned}
                        \beta_n - \beta_{n-1} \> &= \> w_n(x_n - \mu_n)^2 + (\mu_{n-1} - \mu_n) \sum_{i=1}^{n-1} w_i (x_i - \mu_n) \\
                        &= \> w_n(x_n - \mu_n)^2 + (\mu_{n-1} - \mu_n) \left( -w_n(x_n - \mu_n) \right) \\
                        &= \> w_n(x_n - \mu_n)(x_n - \mu_n - \mu_{n-1} + \mu_n) \\
                        &= \> w_n(x_n - \mu_n)(x_n - \mu_{n-1}).
                    \end{aligned}$$
                    Therefore
                    $$\beta_n \> = \> \beta_{n-1} + w_n(x_n - \mu_n)(x_n - \mu_{n-1})$$
                    and
                    $$\begin{aligned}
                        \sigma_n^2 \> &= \> \frac{\beta_n}{W_n} \\
                        &= \> \frac{1}{W_n} \left( \beta_{n-1} + w_n(x_n - \mu_n)(x_n - \mu_{n-1}) \right) \\
                        &= \> \frac{1}{W_n} \left( W_{n-1}\sigma_{n-1}^2 + w_n(x_n - \mu_n)(x_n - \mu_{n-1}) \right) \\
                        &= \> \frac{W_{n-1}}{W_n}\sigma_{n-1}^2 + \frac{w_n}{W_n}(x_n - \mu_n)(x_n - \mu_{n-1}) \\
                        &= \> \left( 1 - \frac{w_n}{W_n} \right)\sigma_{n-1}^2 + \frac{w_n}{W_n}(x_n - \mu_n)(x_n - \mu_{n-1}),
                    \end{aligned}$$
                    which is Welford's method: a formula for the $n^{\text{th}}$ weighted variance with respect
                    to the $(n-1)^{\text{th}}$ weighted variance and the $n^{\text{th}}$ observation (as well
                    as the $(n-1)^{\text{th}}$ and $n^{\text{th}}$ weighted means).
                </p>
                <p>
                    We can generalise Welford's method for the variance $\sigma_n^2$ to the biased, weighted covariance
                    $\rho_n^2$ of the first $n$ observations of $X$ and $Y$, given weighted means $\mu_n$ and $\nu_n$
                    respectively. Skipping the proof of this generalisation, we have that $\rho_n^2 = \frac{\gamma_n}{W_n}$
                    where
                    $$\begin{aligned}
                        \gamma_n \> &= \> \sum_{i=1}^n w_i (x_i - \mu_n)(y_i - \nu_n) \\
                        &= \> \gamma_{n-1} + w_n(x_n - \mu_n)(y_n - \nu_{n-1}) \\
                        &= \> \gamma_{n-1} + w_n(x_n - \mu_{n-1})(y_n - \nu_n).
                    \end{aligned}$$
                    There appears to be an asymmetry between the two possible recursive formulas above, however
                    it can be shown that these two formulas are equivalent (i.e., we can choose to use the current
                    mean for either the $X$ or $Y$ term, and the previous mean for the other). Then
                    $$\begin{aligned}
                        \rho_n^2 \> &= \> \frac{\gamma_n}{W_n} \\
                        &= \> \frac{\gamma_{n-1}}{W_n} + \frac{w_n}{W_n} (x_n - \mu_{n-1})(y_n - \nu_n) \\
                        &= \> \frac{W_{n-1}}{W_n} \rho_{n-1}^2 + \frac{w_n}{W_n} (x_n - \mu_{n-1})(y_n - \nu_n) \\
                        &= \> \left( 1 - \frac{w_n}{W_n} \right) \rho_{n-1}^2 + \frac{w_n}{W_n} (x_n - \mu_{n-1})(y_n - \nu_n),
                    \end{aligned}$$
                    which is Welford's formula for the $n^{\text{th}}$ weighted covariance.
                </p>
                <p>
                    Finally, to compute the unbiased, weighted covariance
                    $$\phi_n^2 \> = \> \frac{W_n}{W_n^2 - \sum_{i=1}^n w_i^2} \sum_{i=1}^n w_i(x_i - \mu_n^2)(y_i - \nu_n^2),$$
                    we can simply multiply the biased, weighted covariance by a correction factor:
                    $$\phi_n^2 \> = \> \frac{W_n^2}{W_n^2 - \sum_{i=1}^n w_i^2} \rho_n^2,$$
                    which can simply be applied to the final covariance result before our algorithm terminates.
                </p>

                <h3>Implementation</h3>
                <p>
                    TODO
                </p>

                <h3>Performance</h3>
                <p>
                    <pre><code class="language-python">
                        def example(a: int, b: float) -> str:
                            return f"The result is {a + b:.2f}"
                    </code></pre>
                </p>
            </section>

            <!-- Footer -->
            <footer>
                <div class="inner">
                    <p style="margin-bottom: 5px;">Copyright &copy; Ethan Mitchell 2024</p>
                    <p style="margin-bottom: 5px;">ethanmitchell98@gmail.com</p>
                </div>
            </footer>

        </div>
    </body>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>

    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/python.min.js"></script>
    <script>hljs.highlightAll();</script>

</html>