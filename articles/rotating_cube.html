<!DOCTYPE HTML>
<html>
    <head>
        <title>Rotating cube from scratch with PyOpenGL</title>
        <meta charset="utf-8" />
        <meta name="description" content="A guide to the mathematics behind 3D graphics and a tutorial on rendering a cube in Python using PyOpenGL.">
        <meta name="author" content="Ethan Mitchell">
        <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
        <link rel="shortcut icon" type="image/png" href="images/e.png"/>

        <link rel="preconnect" href="https://fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;500;700&display=swap" rel="stylesheet">
        <link href="https://fonts.googleapis.com/css2?family=Source+Code+Pro&display=swap" rel="stylesheet">

        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/styles/default.min.css">

        <link rel="stylesheet" href="../style.css?version=6">
    </head>
    <body>
        <div id="wrapper">

            <!-- Title-->
            <section class="article-title">
                <h1>Rotating cube from scratch with PyOpenGL</h1>
                <p class="major">July 2024</p>
                <a id="home-button" href="../index.html">&lt;&nbsp;&nbsp;Home</a>
            </section>

            <!-- Content -->
            <section class="article-body">
                <h3>Context</h3>
                <p>
                    OpenGL is a popular graphics programming specification that is supported by most GPUs used today. OpenGL
                    is implemented via graphics drivers that are typically written in C and provided by the GPU manufacturer,
                    along with client and API libraries, typically written in assembly, C or C++, the latter of which a
                    programmer can use to interact with the GPU. PyOpenGL is a Python binding to the OpenGL API, which is
                    implemented with Python's ctypes foreign function interface and bridges the gap between Python and the
                    lower-level API library, providing a close replica of the OpenGL API in Python.
                </p>
                <p>
                    This article provides a basic overview of the OpenGL API, before detailing the mathematical
                    transformations required to render a 3D cube in a 2D display. This theory is demonstrated via an
                    implementation of the classic rotating cube exercise in PyOpenGL, without relying on GLM, which is OpenGL's
                    mathematics library. This library abstracts the underlying maths by providing a set of functions for
                    constructing transformation matrices, but we will instead implement these transformations ourselves.
                </p>

                <h3>OpenGL Overview</h3>
                <p>
                    The OpenGL API operates like a state machine which we can interact with in one of two ways: by calling
                    a function which changes the state (e.g., linking a new shader program) or by calling a function which
                    performs an action using the current state (e.g., drawing a shape to the screen). This state is called
                    the OpenGL context and is organised like a C struct, where different sub-components (called objects) can
                    be swapped in and out (called binding and unbinding respectively). 
                </p>
                <p>
                    The transformation of 3D objects in memory to 2D pixels on a screen is performed by OpenGL's graphics
                    pipeline, comprised of a sequence of programs called shaders which run on the GPU and are written
                    in GLSL (the openGL Shading Language). Modern GPUs have hundreds or even thousands of cores, each capable
                    of running a shader in parallel, which is why GPUs are so much faster than CPUs for graphics processing
                    (modern CPUs typically have 4-8 far more generalised cores).
                </p>
                <p>
                    The first stage of the graphics pipeline is the vertex shader, for which there is no default (we must
                    provide our own implementation of this step). This shader receives a single vertex as input, whose 3D
                    coordinates $(x,y,z)$ are in "local space", meaning they are relative to the origin of the object the
                    vertex belongs to. This shader is responsible for converting these local coordinates into 3D homogeneous
                    "clip space" coordinates $(x_C, y_C, z_C, w_C)$, which are then used to determine which parts of objects
                    are visible on the screen. The vertex shader also forwards any other data associated with the vertex
                    for use in later stages of the pipeline (such as 2D texture coordinates). The vertex shader achieves
                    the local to clip space conversion by applying a sequence of transformations:
                    <ol>
                        <li>
                            the model transformation converts local coordinates to world coordinates, using information
                            about the object's position, orientation and size in the world we are rendering, and hence
                            is the combination of a scale, translation and rotation transformation;
                        </li>
                        <li>
                            the view transformation converts world coordinates to view/camera coordinates, using information
                            about the position and orientation of the camera in the world, and hence is the combination of a
                            translation and rotation transformation; and
                        </li>
                        <li>
                            the projection transformation converts camera coordinates to clip space coordinates, using
                            information about the camera's type (orthographic or perspective), field of view, aspect ratio
                            and range.
                        </li>
                    </ol>
                </p>
                <p>
                    Following the vertex shader in the graphics pipeline (ignoring some advanced, optional steps) is the primitive
                    assembly stage, where a set of processed vertices are grouped into a sequence of primitives (points, lines or
                    triangles, depending on the context). Next is the clipping stage, where primitives lying outside the viewing
                    volume (defined by $-w_C \le x_C \le w_C$ and likewise for $y_C$ and $z_C$) are discarded, and primitives
                    only partially inside the volume are clipped to fit within it (a triangle may be split into multiple triangles
                    here). Next is the perspective divide
                    $$(x_{\text{NDC}}, y_{\text{NDC}}, z_{\text{NDC}}) \> = \> (\frac{x_C}{w_C}, \frac{y_C}{w_C}, \frac{z_C}{w_C})$$
                    into normalised device coordinates where $|x_{\text{NDC}}| \le 1$, $|y_{\text{NDC}}| \le 1$ and
                    $|z_{\text{NDC}}| \le 1$. After this is the viewport transformation, which converts NDC to window coordinates,
                    which depends on the window size in pixels and the size of the device's depth buffer. Next is face culling,
                    where triangle primitives facing away from view in window space can be discarded, such that they don't need
                    to be rendered. Then the pipeline performs rasterisation, where each primitive is mapped to a set fragments,
                    with at least one fragment per pixel displaying the primitive. A fragment is a collection of all data required
                    to render a single pixel (which includes data that was forwarded from the vertex shader).
                </p>
                <p>
                    The next stage is the fragment shader, which, like the vertex shader, has no default implementation. This shader
                    receives a single fragment as input and returns a colour to be drawn to the screen at its corresponding pixel
                    (as well as some other data including depth). The final stage of the graphics pipeline is a sequence of
                    "per-sample" operations on each fragment, which include pixel ownership testing (which fails if the fragment's
                    pixel location is owned by another overlapping window), depth testing (checking if this fragment is behind
                    another fragment and should be discarded) and blending (combining the fragment's colour with a colour already
                    in the frame buffer for this pixel, to handle opacity).
                </p>

                <h3>Transformation Matrices</h3>
                <p>
                    Before we start programming with PyOpenGL, we first need to define the model, view and projection
                    transformations required to implement our vertex shader. Given a vertex with coordinates $(x,y,z)$, we
                    represent this point in space by the vector of homogeneous coordinates
                    $$\begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix},$$
                    where the fourth coordinate will remain 1 until the projection transformation. Given this definition, we can
                    represent a transformation of this point by a 4x4 matrix that is left-multiplied by the vector. An affine
                    transformation such as a scaling, translation or rotation can be achieved with a matrix multiplication of
                    the form
                    $$
                    \begin{pmatrix}
                        m_{11} & m_{12} & m_{13} & 0 \\
                        m_{21} & m_{22} & m_{23} & 0 \\
                        m_{31} & m_{32} & m_{33} & 0 \\
                        0 & 0 & 0 & 1
                    \end{pmatrix}
                    \begin{pmatrix} x \\ y \\ z \\ 1 \end{pmatrix} \> = \>
                    \begin{pmatrix}
                        m_{11} x + m_{12} y + m_{13} z \\
                        m_{21} x + m_{22} y + m_{23} z \\
                        m_{31} x + m_{32} y + m_{33} z \\
                        1
                    \end{pmatrix},
                    $$
                    and hence combining transformations is equivalent to multiplying their representative matrices.
                    The inclusion of the fourth coordinate may seem pointless, but it allows us to represent translation
                    and perspective projection transformations as matrix multiplications, which would not be possible with
                    3x3 matrices.
                </p>
                <p>
                    A scale transformation with scaling factors $s_X, s_Y$ and $s_Z$ converts a point $(x,y,z,1)$ to the
                    point $(s_X x, s_Y y, s_Z z, 1)$, which is achieved by multiplying by the diagonal matrix
                    $$\begin{pmatrix} s_X & 0 & 0 & 0 \\ 0 & s_Y & 0 & 0 \\ 0 & 0 & s_Z & 0 \\ 0 & 0 & 0 & 1 \end{pmatrix}.$$
                    We can write a Python function which returns this matrix as a 2D NumPy array given a set of scaling factors.
                </p>
                <pre>
                    <code class="language-python">import numpy as np
import numpy.typing as npt


def scale_matrix(s_x: float, s_y: float, s_z: float) -> npt.NDArray:
    assert s_x > 0 and s_y > 0 and s_z > 0, "Input factors must be positive"
    return np.array([
        [s_x, 0.0, 0.0, 0.0],
        [0.0, s_y, 0.0, 0.0],
        [0.0, 0.0, s_z, 0.0],
        [0.0, 0.0, 0.0, 1.0]
    ])</code>
                </pre>
                <br>
                <p>
                    A translation transformation which translates by distance $t_X$ in the $x$-direction, $t_Y$ in the
                    $y$-direction and $t_Z$ in the $z$-direction, thus converting a point $(x,y,z,1)$ to the point
                    $(x + t_X, y + t_Y, z + t_Z, 1)$, is achieved by the matrix:
                    $$\begin{pmatrix} 1 & 0 & 0 & t_X \\ 0 & 1 & 0 & t_Y \\ 0 & 0 & 1 & t_Z \\ 0 & 0 & 0 & 1 \end{pmatrix}.$$
                    This transformation can also be implemented as a simple Python function.
                </p>
                <pre>
                    <code class="language-python">def translation_matrix(t_x: float, t_y: float, t_z: float) -> npt.NDArray:
    return np.array([
        [1.0, 0.0, 0.0, t_x],
        [0.0, 1.0, 0.0, t_y],
        [0.0, 0.0, 1.0, t_z],
        [0.0, 0.0, 0.0, 1.0]
    ])</code>
                </pre>
                <br>
                <p>
                    A rotation in 3D space is defined by a (unit vector) axis and an angle of rotation about that axis. Any rotation
                    in three dimensions can be achieved by a composition of rotations about the standard basis vectors
                    $$
                        \boldsymbol{i} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix}, \>\>
                        \boldsymbol{j} = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix}, \>\>
                        \boldsymbol{k} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}.
                    $$
                    Let's focus on a rotation about the $z$-axis / basis vector $\boldsymbol{k}$ by some acute angle $\theta$, and
                    consider how the other two basis vectors $\boldsymbol{i}$ and $\boldsymbol{j}$ are transformed to vectors
                    $\boldsymbol{i}^{\prime}$ and $\boldsymbol{j}^{\prime}$ respectively.
                </p>
                <p>TODO: DIAGRAM LIKE https://austinmorlan.com/posts/rotation_matrices/media/2d_basis_rotated_triangles.png</p>
                <p>
                    We can form a right-angled triangle in the first quadrant of the $xy$-plane with $\boldsymbol{i}^{\prime}$ as
                    the hypotenuse, its $x$-component the adjacent side and its $y$-component the opposite side. The hypotenuse has
                    unit length because a rotation preserves the length of a vector, and the angle at the origin is $\theta$, and
                    hence we the definitions of sine and cosine give
                    $$
                        \cos(\theta) = \frac{\text{adj}}{\text{hyp}} = \frac{i^{\prime}_x}{1}, \>\>
                        \sin(\theta) = \frac{\text{opp}}{\text{hyp}} = \frac{i^{\prime}_y}{1},
                    $$
                    and of course $i^{\prime}_z = 0$ because $i_z = 0$. Forming a similar right-angled triangle between
                    $\boldsymbol{j}^{\prime}$ and its components (noting that $j^{\prime}_x < 0$), we find that
                    $$
                        \cos(\theta) = \frac{j^{\prime}_y}{1}, \>\>
                        \sin(\theta) = \frac{-j^{\prime}_x}{1},
                    $$
                    and $j^{\prime}_z = 0$. Therefore we have that the acute rotation transforms the standard basis vectors to
                    $$
                        \boldsymbol{i}^{\prime} = \begin{pmatrix} \cos(\theta) \\ \sin(\theta) \\ 0 \end{pmatrix}, \>\>
                        \boldsymbol{j}^{\prime} = \begin{pmatrix} -\sin(\theta) \\ \cos(\theta) \\ 0 \end{pmatrix}, \>\>
                        \boldsymbol{k}^{\prime} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}.
                    $$
                    It can be shown that these results generalise to a rotation by any angle $\theta$. For an obtuse $\theta$, we
                    can draw the above triangles for the acute angle $180^{\deg}-\theta$ between the rotated basis vector and the
                    $x$ and $y$ axes, then using the fact that $\cos(180^{\deg}-\theta) = -\cos(\theta)$ and
                    $\sin(180^{\deg}-\theta) = \sin(\theta)$, we will arrive at the same transformed basis vectors. A similar approach
                    can be used for reflex angles $\theta$, then utilising the fact that angles that are equal modulo $360^{\deg}$
                    produce equivalent rotations, we can claim that the above results apply to any angle $\theta$.
                </p>
                <p>
                    For a general vector $\boldsymbol{v}$, the definition of the standard basis vectors gives us that
                    $$
                        \boldsymbol{v} \> = \> \begin{pmatrix} v_x \\ v_y \\ v_y \end{pmatrix}
                        \> = \> v_x \boldsymbol{i} + v_y \boldsymbol{j} + v_z \boldsymbol{k}
                    $$
                    (i.e. $\boldsymbol{v}$ is a linear combination of the standard basis vectors, with coefficients equal to its
                    entries), and hence the rotation of $\boldsymbol{v}$ about the $z$-axis by angle $\theta$ is
                    $$
                    \boldsymbol{v}^{\prime} \> = \> v_x \boldsymbol{i}^{\prime} + v_y \boldsymbol{j}^{\prime} + v_z \boldsymbol{k}^{\prime}
                    \> = \> \begin{pmatrix} \cos(\theta)v_x - \sin(\theta)v_y \\ \sin(\theta)v_x + \cos(\theta)v_y \\ v_z \end{pmatrix}
                    $$
                    because affine transformations (of which a rotation is an example) preserve linear combinations of vectors.
                    Finally, returning to 3D homogeneous coordinates, we can equate this transformation to multiplication by the
                    matrix
                    $$\begin{pmatrix}
                        \cos \theta & -\sin \theta & 0 & 0 \\
                        \sin \theta & \cos \theta & 0 & 0 \\
                        0 & 0 & 1 & 0 \\
                        0 & 0 & 0 & 1
                    \end{pmatrix}.$$
                    An equivalent derivation can be performed for a rotation about the $x$-axis by an angle $\theta$ to find that its
                    matrix is
                    $$\begin{pmatrix}
                        1 & 0 & 0 & 0 \\
                        0 & \cos \theta & -\sin \theta & 0 \\
                        0 & \sin \theta & \cos \theta & 0 \\
                        0 & 0 & 0 & 1
                    \end{pmatrix},$$
                    and similarly, a rotation about the $y$-axis by $\theta$ is represented by the matrix
                    $$\begin{pmatrix}
                        \cos \theta & 0 & \sin \theta & 0 \\
                        0 & 1 & 0 & 0 \\
                        -\sin \theta & 0 & \cos \theta & 0 \\
                        0 & 0 & 0 & 1
                    \end{pmatrix}.$$
                    We won't derive this nor use it in our implementation, but for the sake of completeness, a rotation about an
                    arbitrary unit vector axis $(r_X,r_Y,r_Z)$ by an angle $\theta$ is represented by the matrix
                    $$
                    \begin{pmatrix}
                        \cos \theta + r_X^2 (1 - \cos \theta) & r_X r_Y (1 - \cos \theta) - r_Z \sin \theta & r_X r_Z (1 - \cos \theta) + r_Y \sin \theta & 0 \\
                        r_Y r_X (1 - \cos \theta) + r_Z \sin \theta & \cos \theta + r_Y^2 (1 - \cos \theta) & r_Y r_Z (1 - \cos \theta) - r_X \sin \theta & 0 \\
                        r_Z r_X (1 - \cos \theta) - r_Y \sin \theta & r_Z r_Y (1 - \cos \theta) + r_X \sin \theta & \cos \theta + r_Z^2 (1 - \cos \theta) & 0 \\
                        0 & 0 & 0 & 1
                    \end{pmatrix},
                    $$
                    which, as mentioned already, can also be achieved by a product of the three basis vector rotations (but this
                    approach can sometimes lead to numerical instability, so many advanced programs using vector rotations prefer
                    to use a direct rotation like this). Given angles of rotation about the $x$, $y$ and $z$ axes, we can write a
                    Python function which returns the resulting rotation matrix product.
                </p>
                <pre>
                    <code class="language-python">def rotation_matrix(
    theta_x_deg: float = 0.0,
    theta_x_rad: float = 0.0,
    theta_y_deg: float = 0.0,
    theta_y_rad: float = 0.0,
    theta_z_deg: float = 0.0,
    theta_z_rad: float = 0.0
) -> npt.NDArray:
    # convert inputs to radians
    assert (theta_x_deg == 0.0) or (theta_x_rad == 0.0), "Either provide theta_x in degrees or radians, not both"
    if theta_x_deg != 0.0:
        theta_x_rad = theta_x_deg * np.pi / 180.0

    assert (theta_y_deg == 0.0) or (theta_y_rad == 0.0), "Either provide theta_y in degrees or radians, not both"
    if theta_y_deg != 0.0:
        theta_y_rad = theta_y_deg * np.pi / 180.0

    assert (theta_z_deg == 0.0) or (theta_z_rad == 0.0), "Either provide theta_z in degrees or radians, not both"
    if theta_z_deg != 0.0:
        theta_z_rad = theta_z_deg * np.pi / 180.0

    # pre-compute sin and cos of theta
    sin = np.sin(theta_rad)
    cos = np.cos(theta_rad)

    # init identity matrix to handle no rotation
    rot_matrix = np.array([
        [1.0, 0.0, 0.0, 0.0],
        [0.0, 1.0, 0.0, 0.0],
        [0.0, 0.0, 1.0, 0.0],
        [0.0, 0.0, 0.0, 1.0]
    ])

    if theta_x_rad != 0.0:
        rot_matrix @= np.array([
            [1.0, 0.0, 0.0, 0.0],
            [0.0, cos, -sin, 0.0],
            [0.0, sin, cos, 0.0],
            [0.0, 0.0, 0.0, 1.0]
        ])
    if theta_y_rad != 0.0:
        rot_matrix @= np.array([
            [cos, 0.0, sin, 0.0],
            [0.0, 1.0, 0.0, 0.0],
            [-sin, 0.0, cos, 0.0],
            [0.0, 0.0, 0.0, 1.0]
        ])
    if theta_z_rad != 0.0:
        rot_matrix @= np.array([
            [cos, -sin, 0.0, 0.0],
            [sin, cos, 0.0, 0.0],
            [0.0, 0.0, 1.0, 0.0],
            [0.0, 0.0, 0.0, 1.0]
        ])

    return rot_matrix</code>
                </pre>
                <br>

                <h3>Model Transformation</h3>
                <p>
                    The model transformation in the vertex shader transforms a vertex's local coordinates to world
                    coordinates, using the position, orientation and scale of the object that the vertex belongs to. Hence,
                    the model transformation matrix is the product of a scale, rotation and translation matrix (in that
                    order, as scaling after translating would also scale the translation). We can use the functions we
                    wrote in the previous section to construct this matrix in Python.
                </p>
                <pre><code class="language-python">def model_matrix(
    scale_x: float = 1.0,
    scale_y: float = 1.0,
    scale_z: float = 1.0,
    rot_x_deg: float = 0.0,
    rot_x_rad: float = 0.0,
    rot_y_deg: float = 0.0,
    rot_y_rad: float = 0.0,
    rot_z_deg: float = 0.0,
    rot_z_rad: float = 0.0,
    trans_x: float = 0.0,
    trans_y: float = 0.0,
    trans_z: float = 0.0
) -> npt.NDArray:
    model = scale_matrix(scale_x, scale_y, scale_z)
    if (
        rot_x_deg != 0.0 or rot_x_rad != 0.0 or
        rot_y_deg != 0.0 or rot_y_rad != 0.0 or
        rot_z_deg != 0.0 or rot_z_rad != 0.0
    ):
        model @= rotation_matrix(rot_x_deg, rot_x_rad, rot_y_deg, rot_y_rad, rot_z_deg, rot_z_rad)
    if trans_x != 0.0 or trans_y != 0.0 or trans_z != 0.0:
        model @= translation_matrix(trans_x, trans_y, trans_z)
    return model</code></pre>
                <br>

                <h3>View Transformation</h3>
                <p>
                    The view transformation in the vertex shader transforms a vertex's world coordinates to view
                    coordinates, using the position and orientation of the camera in the world, in order to render
                    the world from the camera's perspective. Hence, the view transformation matrix is the product
                    of a translation and rotation matrix (in that order, as the rotation axis should stem from the
                    camera's position). Constructing the view matrix in Python is even simpler than the model matrix.
                </p>
                <pre><code class="language-python">def view_matrix(
    camera_x: float = 0.0,
    camera_y: float = 0.0,
    camera_z: float = 0.0,
    rot_x_deg: float = 0.0,
    rot_x_rad: float = 0.0,
    rot_y_deg: float = 0.0,
    rot_y_rad: float = 0.0,
    rot_z_deg: float = 0.0,
    rot_z_rad: float = 0.0
) -> npt.NDArray:
    view = translation_matrix(camera_x, camera_y, camera_z)
    if (
        rot_x_deg != 0.0 or rot_x_rad != 0.0 or
        rot_y_deg != 0.0 or rot_y_rad != 0.0 or
        rot_z_deg != 0.0 or rot_z_rad != 0.0
    ):
        view @= rotation_matrix(rot_x_deg, rot_x_rad, rot_y_deg, rot_y_rad, rot_z_deg, rot_z_rad)
    return view</code>
                </pre>
                <br>

                <h3>Projection Transformation</h3>
                <p>
                    The perspective projection transformation defines a "rectangular frustum" volume in camera space which is
                    indirectly transformed to the $[-1,+1]$ cube in NDC/viewport space. This frustum is a sideways pyramid with
                    a rectangle for a base and its apex cut off (with the base facing away from the camera and the clipped apex
                    pointing towards the camera). We say "indirectly", because this matrix actually transforms from camera space
                    to "clip space" (which still uses 3D homogeneous coordinates), which is a precursor to NDC space. After the
                    vertex shader, OpenGL efficiently performs the perspective divide to convert clip space coordinates to NDC,
                    which completes the transformation from the frustum to the cube.
                </p>
                <p>
                    The inputs defining this transformation are:
                    <ul>
                        <li>
                            the vertical field of view $\theta_y$ which is the angle from the bottom to the top of the view from the
                            camera's origin (equivalently, the angle from the bottom to the top of the frustum from the apex);
                        </li>
                        <li>
                            the aspect ratio $A$ which is the screen width in pixels divided by the screen height in pixels,
                            which is used to derive the horizontal field of view (typical aspect ratios are 16:9 or 4:3);
                        </li>
                        <li>
                            the distance $Z_{\text{near}}$ along the $z$-axis from the camera's origin to the near clipping
                            plane (the close rectangle of the frustum where the apex is cut off) (note this parameter is positive
                            because it is a distance, despite the clipping planes having negative $z$-coordinates in camera space);
                            and
                        </li>
                        <li>
                            the distance $Z_{\text{far}}$ along the $z$-axis from the camera's origin to the far clipping plane
                            (the base of the frustum).
                        </li>
                    </ul>
                </p>
                <p>
                    TODO: DIAGRAM OF FRUSTUM HERE SIDE-VIEW HERE!
                </p>
                <p>
                    To derive the perspective projection matrix, let's forget about clip space for a moment and instead consider
                    the whole transformation from camera space to NDC (i.e., from the frustum to the $[-1,1]$ cube). We will
                    denote camera space coordinates by $x_{\text{cam}}$, $y_{\text{cam}}$ and $z_{\text{cam}}$, and NDC coordinates
                    by $x_{\text{NDC}}$, $y_{\text{NDC}}$ and $z_{\text{NDC}}$.
                </p>
                <p> 
                    Given some $z_{\text{cam}}$ \in $[-Z_{\text{far}}, -Z_{\text{near}}]$ inside the frustum, we can draw a right-
                    angled triangle on the $zy$-plane between the points $(0,0,0)$, $(0,0,z_{\text{cam}})$ and
                    $(0,y_{\text{bdy}},z_{\text{cam}})$, where $y_{\text{bdy}}$ is the half-height of the frustum at this
                    $z$-coordinate (so the third point lies on the top trapezium face of the frustum). The angle inside this triangle
                    at the origin is $\frac{\theta_y}{2}$, and hence we can claim that
                    $$\begin{aligned}
                        \tan\left(\frac{\theta_y}{2}\right) \> &= \> \frac{y_{\text{bdy}}}{-z_{\text{cam}}} \\
                        \Rightarrow \> y_{\text{bdy}} \> &= \> \tan\left(\frac{\theta_y}{2}\right) \cdot -z_{\text{cam}} \\
                        &= \> \frac{-z_{\text{cam}}}{F_y},
                    \end{aligned}$$
                    by defining the constant factor
                    $$F_y \> = \> \frac{1}{\tan\left(\frac{\theta_y}{2}\right)}.$$
                    We have derived an expression for $y_{\text{bdy}}$, which is the half-height of the frustum at any $z_{\text{cam}}$
                    inside the volume. At this $z$-coordinate, the perspective projection will linearly transform
                    $y_{\text{cam}} \in [-y_{\text{bdy}},y_{\text{bdy}}]$ to $y_{\text{NDC}} \in [-1,1]$, and hence
                    $$\begin{aligned}
                        y_{\text{NDC}} &= \frac{y_{\text{cam}}}{y_{\text{bdy}}} \\
                        &= \> y_{\text{cam}} \cdot \frac{F_y}{-z_{\text{cam}}}.
                    \end{aligned}$$
                    We can apply the exact same logic to derive an expression for $x_{\text{NDC}}$, using a modified factor
                    $$F_x \> = \> \frac{F_y}{A},$$
                    where dividing by the aspect ratio accounts for the different width of the frustum along the x-axis, and so
                    $$x_{\text{NDC}} \> = \> x_{\text{cam}} \cdot \frac{F_y}{A \cdot -z_{\text{cam}}}.$$
                    The inclusion of $-z_{\text{cam}}$ on the denominator of this transformation is what causes objects further from
                    the camera to appear smaller on the screen, which is a defining quality of the perspective projection over the
                    orthographic projection.
                </p>
                <p>
                    Now it is left to transform $z_{\text{cam}}$ \in $[-Z_{\text{far}}, -Z_{\text{near}}]$ to
                    $z_{\text{NDC}}$ \in $[-1, 1]$. The standard approach is to use a reciprocal function such that objects further
                    from the camera appear smaller in depth, and it also ensures that $z$-coordinates closer to the camera consume a
                    disproportionately large part of the depth buffer (because objects close to the camera will be focused on more by
                    the viewer, and hence numerical imprecision causing objects that should have slightly different $z$-coordinates to
                    instead overlap would be more problematic if seen up close). Therefore, we establish that
                    $$z_{\text{NDC}} \> = \> \frac{\alpha}{z_{\text{cam}}} + \beta$$
                    for some constants $\alpha$ and $\beta$. We know that $z_{\text{cam}} = -Z_{\text{near}}$ is transformed to the
                    close face of the cube $z_{\text{NDC}} = -1$, and substituting this gives
                    $$\begin{aligned}
                        -1 \> &= \frac{\alpha}{-Z_{\text{near}}} + \beta \\
                        \Rightarrow \> -1 - \beta \> &= \frac{\alpha}{-Z_{\text{near}}} \\
                        \Rightarrow \> \alpha \> &= (-1 - \beta) \cdot -Z_{\text{near}}.
                    \end{aligned}$$
                    Similarly, we know that $z_{\text{cam}} = -Z_{\text{far}}$ is transformed to the far face of the cube
                    $z_{\text{NDC}} = 1$, and so
                    $$\begin{aligned}
                        1 \> &= \frac{\alpha}{-Z_{\text{far}}} + \beta \\
                        \Rightarrow \> 1 - \beta \> &= \frac{\alpha}{-Z_{\text{far}}} \\
                        \Rightarrow \> \alpha \> &= (1 - \beta) \cdot -Z_{\text{far}}.
                    \end{aligned}$$
                    Equating these two expressions for $\alpha$ gives
                    $$\begin{aligned}
                        (-1 - \beta) \cdot -Z_{\text{near}} \> &= \> (1 - \beta) \cdot -Z_{\text{far}} \\
                        \Rightarrow \> Z_{\text{near}} + \beta Z_{\text{near}} \> &= \> -Z_{\text{far}} + \beta Z_{\text{far}} \\
                        \Rightarrow \> \beta Z_{\text{near}} - \beta Z_{\text{far}} \> &= \> -Z_{\text{far}} - Z_{\text{near}} \\
                        \Rightarrow \> \beta \> &= \> -\frac{Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}},
                    \end{aligned}$$
                    which is the constant value of $\beta$. Substituting this back into the second expression for $\alpha$ gives
                    $$\begin{aligned}
                        \alpha \> &= \> (1 - \beta) \cdot -Z_{\text{far}} \\
                        &= \> (1 + \frac{Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}}) \cdot -Z_{\text{far}} \\
                        &= \> -Z_{\text{far}} \frac{Z_{\text{near}} - Z_{\text{far}} + Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}} \\
                        &= \> -\frac{2 Z_{\text{near}} Z_{\text{far}}}{Z_{\text{near}} - Z_{\text{far}}}.
                    \end{aligned}$$
                    Finally, substituting these constants back into our formula for $z_{\text{NDC}}$ gives
                    $$\begin{aligned}
                        z_{\text{NDC}} \> &= \> \frac{\alpha}{z_{\text{cam}}} + \beta \\
                        &= \> - \frac{2 Z_{\text{near}} Z_{\text{far}}}{z_{\text{cam}}(Z_{\text{near}} - Z_{\text{far}})}
                            - \frac{Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}} \\
                        &= \> \frac{-2 Z_{\text{near}} Z_{\text{far}} - z_{\text{cam}}(Z_{\text{far}} + Z_{\text{near}})}
                            {z_{\text{cam}}(Z_{\text{near}} - Z_{\text{far}})} \\
                        &= \> \frac{1}{-z_{\text{cam}}} \left(
                            \frac{Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}} z_{\text{cam}}
                            + \frac{2 Z_{\text{near}} Z_{\text{far}}}{Z_{\text{near}} - Z_{\text{far}}}
                            \right).
                    \end{aligned}$$
                </p>
                <p>
                    We have now derived equations for $x_{\text{NDC}}$, $y_{\text{NDC}}$ and $z_{\text{NDC}}$ in terms of
                    $x_{\text{cam}}$, $y_{\text{cam}}$ and $z_{\text{cam}}$, but notice that these form a non-linear transformation
                    from camera space to NDC/viewport space due to the common divisor of $-z_{\text{cam}}$ in the formulae. This is
                    where the purpose of the intermediate "clip space" becomes evident. As previously defined, NDC are computed from
                    clip space coordinates using the perspective divide operation (which OpenGL performs very efficiently for us):
                    $$
                        x_{\text{NDC}} = \frac{x_{\text{clip}}}{w_{\text{clip}}}, \>
                        y_{\text{NDC}} = \frac{y_{\text{clip}}}{w_{\text{clip}}}, \>
                        z_{\text{NDC}} = \frac{z_{\text{clip}}}{w_{\text{clip}}}.
                    $$
                    If we define $w_{\text{clip}} = -z_{\text{cam}}$, then the NDC equations we derived give us
                    $$\begin{aligned}
                        x_{\text{clip}} \> &= \> \frac{F_y}{A} \cdot x_{\text{cam}}, \\
                        y_{\text{clip}} \> &= \> F_y \cdot y_{\text{cam}}, \\
                        z_{\text{clip}} \> &= \> \frac{Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}} \cdot
                        z_{\text{cam}} + \frac{2 Z_{\text{near}} Z_{\text{far}}}{Z_{\text{near}} - Z_{\text{far}}},
                    \end{aligned}$$
                    all of which are linear equations of the camera coordinates. Therefore, we can define the perspective projection
                    transformation from camera space to clip space as multiplication by the matrix
                    $$
                    \begin{pmatrix}
                        \frac{F_y}{A} & 0 & 0 & 0 \\
                        0 & F_y & 0 & 0 \\
                        0 & 0 & \frac{Z_{\text{far}} + Z_{\text{near}}}{Z_{\text{near}} - Z_{\text{far}}} &
                        \frac{2 Z_{\text{near}} Z_{\text{far}}}{Z_{\text{near}} - Z_{\text{far}}} \\
                        0 & 0 & -1 & 0
                    \end{pmatrix}.
                    Finally, we can implement a simple Python function that returns this matrix given the field of view, aspect ratio,
                    and the near and far clipping planes.
                </p>
                <pre>
                    <code class="language-python">def projection_matrix(
    fov_y_deg: float = 0.0,
    fov_y_rad: float = 0.0,
    aspect_ratio: float = 4/3,
    z_near: float = 0.1,
    z_far: float = 10.0
) -> npt.NDArray:
    assert aspect_ratio > 0.0, "aspect_ratio must be > 0"
    assert 0 <= z_near < z_far, "z_near must be >= 0 and z_far must be > z_near"

    # convert input to radians
    assert (fov_y_deg <= 0.0) != (fov_y_rad <= 0.0), "Exactly one field of view input must be provided"
    if fov_y_rad == 0.0:
        fov_y_rad = fov_y_deg * np.pi / 180.0

    factor_y = 1 / (np.tan(fov_y_rad / 2))

    return np.array([
        [factor_y / aspect_ratio, 0.0, 0.0, 0.0],
        [0.0, factor_y, 0.0, 0.0],
        [0.0, 0.0, (z_far + z_near) / (z_near - z_far), (2 * z_near * z_far) / (z_near - z_far)],
        [0.0, 0.0, -1.0, 0.0]
    ])</code>
                </pre>

                <h3>PyOpenGL Implementation</h3>
                <p>
                    TODO: intro, then shader code
                </p>
                <p>
                    Now that we have defined our shaders, we can implement a utility function which loads a shader's source code from file, creates an OpenGL "shader
                    object" for it, attaches the source code and compiles it, then returns the shader object's ID. This function also checks for compilation errors,
                    such that this utility function successfully returning implies that the shader's source code is valid. The arguments to this function are simply
                    the file path of the shader's source code and the type of shader (either "vertex" or "fragment").
                </p>
                <pre>
                    <code class="language-python">from OpenGL.GL import *
from pathlib import Path


def load_shader(file_name: str, shader_type: str = "vertex") -> int:
    if not Path(file_name).exists():
        raise FileNotFoundError(f"{shader_type.capitalize()} shader file not found: {file_name}")

    with open(file_name) as f:
        shader_src = f.read()
    
    shader_id = glCreateShader(GL_VERTEX_SHADER if shader_type == "vertex" else GL_FRAGMENT_SHADER)
    glShaderSource(shader_id, shader_src)
    glCompileShader(shader_id)

    # ensure that the shader compiled successfully
    success = glGetShaderiv(shader_id, GL_COMPILE_STATUS)
    if not success:
        raise RuntimeError(f"Failed to compile {shader_type} shader: {glGetShaderInfoLog(shader_id).decode()}")

    return shader_id</code>
                </pre>

                <h3>References</h3>
                <p>TODO</p>
                https://www.khronos.org/opengl/wiki/Rendering_Pipeline_Overview
                https://learnopengl.com/Getting-started/OpenGL
                https://austinmorlan.com/posts/rotation_matrices/
                https://unspecified.wordpress.com/2012/06/21/calculating-the-gluperspective-matrix-and-other-opengl-matrix-maths/
            </section>

            <!-- Footer -->
            <footer>
                <div class="inner">
                    <p style="margin-bottom: 5px;">Copyright &copy; Ethan Mitchell 2024</p>
                    <p style="margin-bottom: 5px;">ethanmitchell98@gmail.com</p>
                </div>
            </footer>

        </div>
    </body>

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            }
        };
    </script>
    <script async src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.min.js"></script>

    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/highlight.min.js"></script>
    <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@11.9.0/build/languages/python.min.js"></script>
    <script>hljs.highlightAll();</script>

</html>